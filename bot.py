from dotenv import load_dotenv
import os
import nest_asyncio
import requests
import threading
import asyncio
from flask import Flask
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackContext
from transformers import pipeline

# ‚úÖ Load environment variables
load_dotenv()
TOKEN = os.getenv("BOT_TOKEN")
HF_API_KEY = os.getenv("HF_API_KEY")

# ‚úÖ Ensure BOT_TOKEN exists
if not TOKEN:
    raise ValueError("‚ö†Ô∏è BOT_TOKEN is missing! Check environment variables.")

# ‚úÖ Initialize Flask app
app = Flask(__name__)

@app.route('/')
def home():
    return "Bot is running!"

# ‚úÖ Track bot status
bot_active = False  # Default OFF

# ‚úÖ AI def chatbot_response
    (user_input):"Handles AI responses via Hugging Face API, Together AI, or local model."
    if not bot_active:
        return "‚ùå Bot is inactive. Use /startbot to activate."

    # ‚úÖ Try Hugging Face API first
    if HF_API_KEY:
        headers = {"Authorization": f"Bearer {HF_API_KEY}"}
        data = {"inputs": user_input}
        response = requests.post(
            "https://api-inference.huggingface.co/models/facebook/blenderbot-400M-distill",
            json=data, headers=headers
        )
        print(f"üîπ Hugging Face API Response: {response.status_code} - {response.text}")

        if response.status_code == 200:
            result = response.json()
            if isinstance(result, list) and len(result) > 0 and isinstance(result[0], dict):
                return result[0].get("generated_text", "I couldn't process that.")
            return "‚ö†Ô∏è Unexpected response format from Hugging Face."
        print(f"‚ö†Ô∏è Hugging Face API Error: {response.status_code} - {response.text}")

    # ‚úÖ Fallback to Together AI with API Key
    if TOGETHER_AI_KEY:
        together_url = "https://api.together.xyz/inference"
        headers = {
            "Authorization": f"Bearer {TOGETHER_AI_KEY}",
            "Content-Type": "application/json"
        }
        data = {"model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "prompt": user_input, "max_tokens": 300}
        
        try:
            response = requests.post(together_url, json=data, headers=headers)
            print(f"üîπ Together AI API Response: {response.status_code} - {response.text}")

            if response.status_code == 200:
                result = response.json()
                if isinstance(result, dict):
                    return result.get("text", "I couldn't process that.")
                return "‚ö†Ô∏è Unexpected AI response format."
            print(f"‚ö†Ô∏è Together AI Error: {response.status_code} - {response.text}")
        except Exception as e:
            print(f"‚ö†Ô∏è Together AI Request Error: {str(e)}")

    # ‚úÖ Final fallback: Local AI Model
    print("‚ö†Ô∏è Falling back to local AI model...")
    local_chatbot = pipeline("text-generation", model="microsoft/DialoGPT-medium")
    response = local_chatbot(user_input, max_length=100, pad_token_id=50256)
    return response[0]['generated_text']


# ‚úÖ Start the bot manually
async def startbot(update: Update, context: CallbackContext):
    global bot_active
    bot_active = True
    await update.message.reply_text("‚úÖ Bot is now active! You can chat and use commands.")

# ‚úÖ Stop the bot manually
async def stopbot(update: Update, context: CallbackContext):
    global bot_active
    bot_active = False
    await update.message.reply_text("‚ùå Bot has been deactivated. Use /startbot to activate again.")

# ‚úÖ Handle user messages
async def chat(update: Update, context: CallbackContext):
    user_text = update.message.text.strip()
    response = chatbot_response(user_text)
    await update.message.reply_text(response)

# ‚úÖ Initialize Telegram Bot
bot_app = Application.builder().token(TOKEN).build()
bot_app.add_handler(CommandHandler("startbot", startbot))
bot_app.add_handler(CommandHandler("stopbot", stopbot))
bot_app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, chat))

print("‚úÖ Bot is ready!")
print("üåç Running on macOS/Linux-compatible setup")

# ‚úÖ Run Flask & Telegram Bot Together
def run_flask():
    """Run Flask app on a separate thread."""
    from waitress import serve  # macOS-compatible production server
    serve(app, host="0.0.0.0", port=5000)

# Start Flask in a separate thread
flask_thread = threading.Thread(target=run_flask, daemon=True)
flask_thread.start()

# ‚úÖ Start Telegram bot polling (Fixes macOS asyncio issue)
nest_asyncio.apply()
asyncio.run(bot_app.run_polling())
